
## Code Explanation and File Description

### 1. `app.py`
**Purpose**: This is the main script to launch the chatbot. It integrates user interaction, LLM (Large Language Model) responses, and optional document retrieval for RAG (Retrieval-Augmented Generation)-based queries.

**Detailed Explanation**:
- **Libraries Imported**:
  ```python
  import streamlit as st
  import os
  from dotenv import load_dotenv
  from langchain.chains import LLMChain
  from langchain.prompts import ChatPromptTemplate
  from langchain.llms import OpenAI
  from langchain.vectorstores import FAISS
  from langchain.document_loaders import PyPDFDirectoryLoader
  from langchain.text_splitter import RecursiveCharacterTextSplitter
  ```
- **Functionality**:
  - Streamlit creates the user-friendly interface.
  - LangChain orchestrates LLM responses and integrates RAG.
  - FAISS enables document retrieval for enhanced context.

---

### 2. `environment.py`
**Purpose**: Loads environment variables required for the chatbot's configuration and API access.

**Code**:
```python
import os
from dotenv import load_dotenv

def load_env():
    load_dotenv()
    return {
        "LANGCHAIN_API_KEY": os.getenv("LANGCHAIN_API_KEY"),
        "GROQ_API_KEY": os.getenv("GROQ_API_KEY"),
        "HF_TOKEN": os.getenv("HF_TOKEN"),
    }
```

---

### 3. `llm_metrics_evaluation.py`
**Purpose**: Evaluates the performance of three LLMs (Llama, Mixtral, and Gemma) using various metrics.

**Detailed Explanation**:
- **Libraries Imported**:
  ```python
  import pandas as pd
  from sklearn.metrics.pairwise import cosine_similarity
  from nltk.translate.bleu_score import sentence_bleu
  from rouge_score import rouge_scorer
  ```

- **Workflow**:
  - **Data Loading**:
    - Loads CSV files containing:
      - Gold-standard answers.
      - Answers generated by Llama, Mixtral, and Gemma.
  - **Metrics**:
    - BLEU, ROUGE-L, Cosine Similarity, BERTScore, Accuracy.
  - **Output**:
    - Results are saved in `comparison_results_weighted_RAG.csv`.

---

### 4. `T-Test Analysis for LLM Comparisons.py`
**Purpose**: Performs statistical tests (paired T-tests) to determine significant differences between the models.

**Code**:
```python
from scipy.stats import ttest_rel

# Example function for paired T-test
def perform_ttest(model_a_scores, model_b_scores):
    t_stat, p_value = ttest_rel(model_a_scores, model_b_scores)
    return t_stat, p_value
```

---

### 5. `Weighted average of all models.csv`
**Purpose**: Contains the weighted average scores of all models for 96 questions.

**Detailed Explanation**:
- **Columns Include**:
  - Weighted averages for Base Llama, Mixtral, Gemma, and their RAG versions.

---

### 6. `Evaluation Metrics results.csv`
**Purpose**: Stores detailed metrics (BLEU, ROUGE-L, Cosine Similarity, bert score, accuracy) for each model and question.

---

### 7. `T-Test Results.pdf`
**Purpose**: Summarizes the results of all model comparisons with tables for:
- T-statistic values.
- P-values.
- Statistical significance.
